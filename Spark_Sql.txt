1. we can create a table view using df- 
    df.createOrReplaceTempView('table name')
    new df = spark.sql('select * from table name') //temp view table can be accessed using spark.sql
2. we can convert table / view to df using - 
    df = spark.read.table('orders') or 
    df = spark.sql('') or
    df = spark.table('table name')

External tables:
Data is stored in the Data Lake (S3/ADLS/etc).
Metadata is stored in Hive Metastore.
Dropping the table removes only metadata, data remains.

Managed tables:
Data is stored in /user/hive/warehouse/<db>.db/<table>/.
Metadata is stored in Hive Metastore.
Dropping the table removes both data and metadata.

| Table type | Data location                 | Metadata  | Drop table behavior |
| ---------- | ----------------------------- | --------- | ------------------- |
| Managed    | HDFS (`/user/hive/warehouse`) | Metastore | Data deleted        |
| External   | Any HDFS path                 | Metastore | Data preserved      |

Very Imp checklist - 
| Situation                | Overwrite path directly? |
| ------------------------ | ------------------------ |
| Path only, no table      | ✅ Yes                    |
| External table exists    | ❌ No                     |
| Table dropped            | ✅ Yes                    |
| Writing via `insertInto` | ✅ Yes                    |

Issue faced in project overwrite the path with new files as external table is created hence
    1. either create a new path and then drop the table and point to new path
    2. drop the table first update the same path and create the same table again - best if production

Also note cannot overwrite the external table or file pointed out by external table we need to drop first