1. we can create a table view using df- 
    df.createOrReplaceTempView('table name')
    new df = spark.sql('select * from table name') //temp view table can be accessed using spark.sql
2. we can convert table / view to df using - 
    df = spark.read.table('orders') or 
    df = spark.sql('') or
    df = spark.table('table name')

External tables:
Data is stored in the Data Lake (S3/ADLS/etc).
Metadata is stored in Hive Metastore.
Dropping the table removes only metadata, data remains.

Managed tables:
Data is stored in /user/hive/warehouse/<db>.db/<table>/.
Metadata is stored in Hive Metastore.
Dropping the table removes both data and metadata.

| Table type | Data location                 | Metadata  | Drop table behavior |
| ---------- | ----------------------------- | --------- | ------------------- |
| Managed    | HDFS (`/user/hive/warehouse`) | Metastore | Data deleted        |
| External   | Any HDFS path                 | Metastore | Data preserved      |

Very Imp checklist - 
| Situation                | Overwrite path directly? |
| ------------------------ | ------------------------ |
| Path only, no table      | ✅ Yes                    |
| External table exists    | ❌ No                     |
| Table dropped            | ✅ Yes                    |
| Writing via `insertInto` | ✅ Yes                    |

Issue faced in project overwrite the path with new files as external table is created hence
    1. either create a new path and then drop the table and point to new path
    2. drop the table first update the same path and create the same table again - best if production

Also note cannot overwrite the external table or file pointed out by external table we need to drop first

Final pointers very imp:-
1. hive has managed and external tables
2. managed tables can be created in default dir /user/hive/arehouse this is where data is present to avoid data cluttering avoid writting it here 
   write it in /user/id/warehouse using spark.sql.warehouse.dir
3. metadata is still in metasotre in db not issues here as no concept of multiple catalogs like DATABRICKS
4. external table and managed tbles in hive both can only have insert and select properties - Y? because insertion creates new files
   managed - new files created at /user/id/warehouse/db/tablename/newfile -added here
   external - new files created at /external file path/new files added here
   they are diff from delta tables in DB as DB supports smooth metadata managemet.
5. finally very imp all the file formats in spark are immutable you might have seen when using multiple modes in spark while writing the data
   that works only at folder level hence all types are immutable even in DB delta lake uses metadata to track changes.
 