1. we can create a table view using df- 
    df.createOrReplaceTempView('table name')
    new df = spark.sql('select * from table name') //temp view table can be accessed using spark.sql
2. we can convert table / view to df using - 
    df = spark.read.table('orders') or 
    df = spark.sql('') or
    df = spark.table('table name')

External tables:
Data is stored in the Data Lake (S3/ADLS/etc).
Metadata is stored in Hive Metastore.
Dropping the table removes only metadata, data remains.

Managed tables:
Data is stored in /user/hive/warehouse/<db>.db/<table>/.
Metadata is stored in Hive Metastore.
Dropping the table removes both data and metadata.

| Table type | Data location                 | Metadata  | Drop table behavior |
| ---------- | ----------------------------- | --------- | ------------------- |
| Managed    | HDFS (`/user/hive/warehouse`) | Metastore | Data deleted        |
| External   | Any HDFS path                 | Metastore | Data preserved      |

Very Imp checklist - 
| Situation                | Overwrite path directly? |
| ------------------------ | ------------------------ |
| Path only, no table      | ✅ Yes                    |
| External table exists    | ❌ No                     |
| Table dropped            | ✅ Yes                    |
| Writing via `insertInto` | ✅ Yes                    |

Issue faced in project overwrite the path with new files as external table is created hence
    1. either create a new path and then drop the table and point to new path
    2. drop the table first update the same path and create the same table again - best if production

Also note cannot overwrite the external table or file pointed out by external table we need to drop first

Final pointers very imp:-
1. hive has managed and external tables
2. managed tables can be created in default dir /user/hive/arehouse this is where data is present to avoid data cluttering avoid writting it here 
   write it in /user/id/warehouse using spark.sql.warehouse.dir
3. metadata is still in metasotre in db not issues here as no concept of multiple catalogs like DATABRICKS
4. external table and managed tbles in hive both can only have insert and select properties - Y? because insertion creates new files
   managed - new files created at /user/id/warehouse/db/tablename/newfile -added here
   external - new files created at /external file path/new files added here
   they are diff from delta tables in DB as DB supports smooth metadata managemet.
5. finally very imp all the file formats in spark are immutable you might have seen when using multiple modes in spark while writing the data
   that works only at folder level hence all types are immutable even in DB delta lake uses metadata to track changes.
6. very imp note about msck repair table tablename - 
   1. this works for both managed and external table what it does is it only deals with partitioning if a table has 
      and will check the new folder or files and add the new files when spark write the data to folder directly and then this command refreshes the table
   2. this is does not make sense when spark write the data to table directly becaue in first case the spark did bypass the hive metastore but while writing the data to table it did use hive metastoe.
7. hive is schema on read very imp that means ex:- data has 5 cols but hive table schema has 4 cols hence user when select * sees only 4 cols
8. plus acid is allowed only when table in in orc format + transaction enabled.